---
sidebar_position: 2
sidebar_label: 部署手册
---

# 一、单机部署
## 基础环境依赖

 - docker == 1.27.x   docker 存储目录>1T
 - kubernetes = 1.21~1.25
 - kubectl == 1.24 
 - nfs/ceph等分布式文件系统 挂载到每台机器的 /data/k8s/ （单机可忽略）
 - 数据库接口地址 mysql，没有可忽略使用cube-studio自带的
 - 单机 磁盘>=1000G 单机磁盘容量要求不大，仅做镜像容器的的存储  
 - 控制端机器1-2台 cpu>=16 mem>=32G  
 - 任务端cpu/gpu机器	根据需要自行配置和扩容，gpu安装对应厂商的要求安装好机器驱动
 - IB/RDMA网络	自动安装机器驱动和IB卡
 - 系统 ubuntu 20.04 ubuntu 22.04 或者centos7或者centos8

平台完成部署之后如下:

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d769fc3d67d84820b2a2fd7f2cc6ad93.png)

## 部署提前预备内容

1、分布式存储，挂载到/data/k8s目录下（单机可忽略）。

nfs分布式存储示例：参考cube-studio/install/kubernetes/nfs/NFS离线部署.md

2、私有镜像仓库，在每台机器的docker配置中添加Insecure Registries(体验可忽略)

参考：cube-studio/install/harbor/readme.md

3、gpu机器要安装nvidia-docker2，gpu机器安装对应的驱动，IB设备也需要安装对应的驱动(cpu机器可忽略)

参考：cube-studio/install/rancher

### 1、未有k8s的用户

可以参考视频，先使用rancher部署k8s，再部署cube-studio

[单机部署视频](https://www.bilibili.com/video/BV18r4y147oj/)

也可以查看使用rancher部署k8s的文档，可以查看cube-studio/install/kubernetes/rancher/readme.md

### 2、对于已有k8s的用户

1、对于ipvs模式的k8s，
  （1）要将start.sh脚本最后面的`kubectl patch`注释掉。然后手动释放istio-system命名空间 istio-ingressgateway,服务类型改为NodePort。
  （2）将配置文件install/kubernetes/cube/overlays/config/config.py中的K8S_NETWORK_MODE 改为ipvs

2、服务可用端口范围要放大到10~60000

3、平台入口是istio-system命名空间 istio-ingressgateway，不是cube-studio的前端部分

平台部署命令：

将k8s集群的kubeconfig文件复制到install/kubernetes/config文件中,对于双网卡的同学，记得rancher里面current-context切换为内网的连接形式，然后执行如下命令，其中xx.xx.xx.xx为机器内网的ip（不是外网ip）
```
# 在k8s worker机器上执行
sh start.sh xx.xx.xx.xx
```

kubeconfig文件位置

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/20677dc5fefb41ca99969144d158e721.png)

### 3、隔离内网部署

需要先把需要的镜像拉取到内网docker仓库，再从内网docker仓库拉取到机器上。

 - [内网拉取rancher所需镜像(自有k8s可忽略)](../../install/kubernetes/rancher/all_image.py )
 - [内网拉取cube-studio所需镜像](../../install/kubernetes/all_image.py)

其他的同第一步或第二步

将infra命名空间下deployment拉取策略改为IfNotPresent，同时修改[配置文件](../运维/自定义配置文件.md)中IMAGE_PULL_POLICY为IfNotPresent


4、创建企业自己的镜像仓库，在web界面，在线开发-镜像仓库，添加自己的仓库地址和账号密码。注意hubsecret不要删除

5、修改config.py文件中REPOSITORY_ORG为自己的镜像仓库地址，HUBSECRET添加自己上一步配置的k8s hubsecret，并将配置更新到infra/kubeflow-dashboard-config的configmap中。

## 修改为非80端口

修改istio-system命名空间下面名为istio-ingressgateway的service，将服务类型改为NodePort。将80端口的nodePort改为其他可以访问的端口，注意不要删除80端口的配置，因为在gateway中会读取这个80参数。

## 域名访问

修改 install/kubernetes/gateway.yaml,将其中的host参数 *  修改为域名。然后重新部署这个gateway.yaml文件

修改域名后notebook要重新reset才能打开。

## 内部ip和外部ip

部署cube-studio时，start.sh脚本后面要填写内网ip，浏览器访问使用外网ip（公网ip），修改[config.py](../运维/自定义配置文件.md)中SERVICE_EXTERNAL_IP为`['内网ip|外网ip']`。

## nginx 反向代理istio-ingressgateway

有时候浏览器只能打开物理机ip，而我们部署cube-studio使用的是虚拟机。需要添加nginx代理，注意nginx访问超时时常要设置大一些，不然websocket协议的webshell执行命令会失败

nginx配置参考：myapp/install/kubernetes/nginx-https/nginx.conf


## 部署后排查

0、在内网中会存在，kubectl工具不存在，无法下载，可以查看修改下start.sh脚本，手动下载kubectl。

1、如果k8s采用的是ipvs网络模式，修改istio-ingressgateway为nodeport模式，而不是externalIPs模式，并在配置文件config.py中将iptables修改为ipvs

2、查看机器lable是否添加
```
kubectl label node $node train=true cpu=true notebook=true service=true org=public istio=true knative=true kubeflow=true kubeflow-dashboard=true mysql=true redis=true monitoring=true logging=true --overwrite
```

3、pv和pvc是否绑定。有些k8s平台会自动为pvc添加Storage Classes，需要对应pv也添加完成绑定
  
4、kube-system命名空间coredns是否正常，
 - 4.1、可以放开防火墙
```
/sbin/iptables -P FORWARD ACCEPT
/sbin/iptables -P INPUT ACCEPT
/sbin/iptables -P OUTPUT ACCEPT
```
放开防火墙后重启coredns 的pod
 - 4.2、可以查看53端口是否被占用，比如bind服务，关闭主机原有53端口服务(注意机器重启后可能原53端口占用的服务被重新打开)。

5、infra命名空间其他组件是否成功，会链接mysql完成数据库初始化。kubeflow数据库由kubeflow-dashboard.infra组件完成初始化。
 - 5.1 数据库如果库表不全，可以把对应库删掉，重启相应组件。rm -rf /data/k8s/infra/mysql，然后重启mysql，再重启kubeflow-dashboard
 - 5.2  如果kubeflow-dashboard报core dump，则可能是linux系统内核版本太低，需要升级系统内核

6、istio-system空间，istio-ingressgateway的svc是否externalIPs形式暴露了内网ip。 如果有双网卡， 在执行sh start.sh xx.xx.xx.xx时，使用的是内网ip，浏览器打开的是外网ip

6、istio-ingressgateway暴露80以后，仍然无法打开，可能原因（1）80被内网封禁（2）80倍其他服务占用（3）没有禁用nginx-ingress (4)防火墙限制，可以`/sbin/iptables -P FORWARD ACCEPT`

7、istio-ingressgateway暴露其他端口，如果80端口被占用了，可以修改 istio-system/svc/istio-ingressgateway 类型为NodePort

8、镜像拉取不下来：可以离线拉取一遍[平台镜像](../../install/kubernetes/pull_image_kubeflow.sh)，对于其他的仓库镜像，最好public，否则更新每个命名空间hubsecret的账号密码。配置在线开发->镜像仓库 中hubsecret中的账号密码为自己的dockerhub官网的账号密码


内网机器需要安装了docker，docker-compose，iptables

# 二、内网离线部署

## 1.内网中有可以联网的机器

###  联网机器设置代理服务器

联网机器上设置nginx代理软件源，参考install/kubernetes/nginx-https/apt-yum-pip-source.conf

启动nginx代理访问

需要监听80和443端口
```bash
docker run --name proxy-repo -d --restart=always --network=host -v $PWD/nginx-https/apt-yum-pip-source.conf:/etc/nginx/nginx.conf nginx 
```

### 在内网机器上配置host

host
```bash
<出口服务器的IP地址>    mirrors.aliyun.com
<出口服务器的IP地址>    ccr.ccs.tencentyun.com
<出口服务器的IP地址>    registry-1.docker.io
<出口服务器的IP地址>    auth.docker.io
<出口服务器的IP地址>    hub.docker.com
<出口服务器的IP地址>    www.modelscope.cn
<出口服务器的IP地址>    modelscope.oss-cn-beijing.aliyuncs.com
<出口服务器的IP地址>    archive.ubuntu.com
<出口服务器的IP地址>    security.ubuntu.com
<出口服务器的IP地址>    cloud.r-project.org
<出口服务器的IP地址>    deb.nodesource.com
<出口服务器的IP地址>    docker-76009.sz.gfp.tencent-cloud.com
```

添加新的host要重启下kubelet   docker restart kubelet

如果代理机器没法占用80和443，需要使用iptable尝试转发。

iptables
```bash
sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -d mirrors.aliyun.com -j DNAT --to-destination <出口服务器的IP地址>:<出口服务器的端口>
```

### k8s配置域名解析

k8s中修改 kube-system命名空间，coredns的configmap，添加 需要访问的地址 的地址映射
```bash
{
  "Corefile": ".:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        # 自定义host
        hosts {
            <出口服务器的IP地址>    mirrors.aliyun.com
                <出口服务器的IP地址>    ccr.ccs.tencentyun.com
                <出口服务器的IP地址>    registry-1.docker.io
                <出口服务器的IP地址>    auth.docker.io
                <出口服务器的IP地址>    hub.docker.com
                <出口服务器的IP地址>    www.modelscope.cn
                <出口服务器的IP地址>    modelscope.oss-cn-beijing.aliyuncs.com
                <出口服务器的IP地址>    archive.ubuntu.com
                <出口服务器的IP地址>    security.ubuntu.com
                <出口服务器的IP地址>    cloud.r-project.org
                <出口服务器的IP地址>    deb.nodesource.com
                <出口服务器的IP地址>    docker-76009.sz.gfp.tencent-cloud.com
          fallthrough
        }
        prometheus :9153
        forward . \"/etc/resolv.conf\"
        cache 30
        loop
        reload
        loadbalance
    } # STUBDOMAINS - Rancher specific change
    "
}
```
重启coredns的pod

### 容器里面使用放开的域名

pip配置https源:
```bash
pip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple
```

apt配置https源: 修改/etc/apt/source.list

ubuntu 20.04
```bash

deb https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse

deb https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse

deb https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse

deb https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse

deb https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
deb-src https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse
```

yum 配置https源：下载阿里的源
```bash
wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-8.repo
```

## 2.完全无法联网的内网机器

### 安装依赖组件和数据

能外网的机器上，拷贝到内网机器上
````bash
mkdir offline
cd offline
wget https://cube-studio.oss-cn-hangzhou.aliyuncs.com/install/kubectl
wget https://cube-studio.oss-cn-hangzhou.aliyuncs.com/harbor/harbor-offline-installer-v2.3.4.tgz
# 下载模型
wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/resnet50.onnx
wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/resnet50-torchscript.pt
wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/resnet50.mar
wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/tf-mnist.tar.gz

# 训练,标注数据集
wget https://cube-studio.oss-cn-hangzhou.aliyuncs.com/pipeline/coco_data_sample.zip
wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/aihub/deeplearning/cv-tinynas-object-detection-damoyolo/dataset/coco2014.zip

````

连不上网的机器上

1、安装kubectl
```bash
chmod +x kubectl  && cp kubectl /usr/bin/ && mv kubectl /usr/local/bin/
```

2、[安装内网镜像仓库](harbor/readme.md)

参考install/kubernetes/harbor/readme.md

并创建cube-studio和rancher项目，分别存放rancher的基础镜像和cube-studio的基础镜像

配置每台机器docker添加这个 insecure-registries内网的私有镜像仓，如果是https可以忽略

参考： install/kubernetes/rancher/install_docker.md

3、将其他前面下载的数据转移到个人目录下

```bash
cp -r offline /data/k8s/kubeflow/pipeline/workspace/admin/
```

### 镜像转移至内网

### 转移rancher镜像

修改install/kubernetes/rancher/all_image.py中内网仓库地址，运行导出推送和拉取脚本.

联网机器上运行 pull_rancher_images.sh将镜像推送到内网仓库 或 rancher_image_save.sh将镜像压缩成文件再导入到内网机器

不能联网机器上运行，每台机器运行 pull_rancher_harbor.sh 从内网仓库中拉取镜像 或 rancher_image_load.sh 从压缩文件中导入镜像

### 内网部署 k8s

使用rancher相同方法可在内网部署k8s

### 转移cube-studio基础镜像

修改all_image.py中内网仓库地址，运行导出推送和拉取脚本.

联网机器上运行 push_harbor.sh 将镜像推送到内网仓库 或 image_save.sh将镜像压缩成文件再导入到内网机器

不能联网机器上运行，每台机器运行 pull_harbor.sh 从内网仓库中拉取镜像 或 image_load.sh 从压缩文件中导入镜像

### 构建内网版本cube-studio

联网机器上，重新打前后端镜像，并更新到内网仓库

在install/kubernetes目录下执行 替换成内网镜像

```python3
cube_repo = '<内网镜像仓库ip>:<内网镜像仓库端口>/cube-studio/'
import os
def fix_file(file_path):
    if os.path.isdir(file_path):
        file_paths = [os.path.join(file_path, one) for one in os.listdir(file_path)]
    else:
        file_paths = [file_path]
        
    for file_path in file_paths:
        content = ''.join(open(file_path, mode='r').readlines())
        content = content.replace('ccr.ccs.tencentyun.com/cube-studio/', cube_repo)  # 替换自产镜像
        content = content.replace('docker:23.0.4', cube_repo + 'docker:23.0.4')  # 替换docker
        content = content.replace('python:', cube_repo + 'python:')  # 替换docker
        file = open(file_path, mode='w')
        file.write(content)
        file.close()

fix_file('cube/overlays/config/config.py')
fix_file('../../myapp/init')
fix_file('../../myapp/init-en')
```

项目根路径下
```bash
构建前端
docker build --network=host -t 192.168.3.7:88/cube-studio/kubeflow-dashboard-frontend-enterprise:2024.01.01-offline -f install/docker/dockerFrontend/Dockerfile .
docker push 192.168.3.7:88/cube-studio/kubeflow-dashboard-frontend-enterprise:2024.01.01-offline
构建后端
docker build --network=host -t 192.168.3.7:88/cube-studio/kubeflow-dashboard-enterprise:2024.01.01-offline --build-arg TARGETARCH=amd64 -f install/docker/Dockerfile .
docker push 192.168.3.7:88/cube-studio/kubeflow-dashboard-enterprise:2024.01.01-offline
```

### 内网部署cube-studio

1、修改init_node.sh中pull_images.sh 修改为pull_harbor.sh，表示从内网拉取镜像，每台机器都要执行。

2、取消下载kubectl，注释掉
```bash
ARCH=$(uname -m)

if [ "$ARCH" = "x86_64" ]; then
  wget https://cube-studio.oss-cn-hangzhou.aliyuncs.com/install/kubectl && chmod +x kubectl  && cp kubectl /usr/bin/ && mv kubectl /usr/local/bin/
elif [ "$ARCH" = "aarch64" ]; then
  wget -O kubectl https://cube-studio.oss-cn-hangzhou.aliyuncs.com/install/kubectl-arm64 && chmod +x kubectl  && cp kubectl /usr/bin/ && mv kubectl /usr/local/bin/
fi
```
3、修改cube-studio镜像为内网镜像。
```bash
vi cube/overlays/kustomization.yml
修改最底部的newName和newTag
```

3、复制k8s的config文件，部署cube-studio，部署方式通外网，参考：部署/单机部署

### web界面的部分内网修正

1、web界面hubsecret改为内部仓库的账号密码

2、自带的目标识别pipeline中，第一个数据拉取任务启动命令改为，`cp offline/coco_data_sample.zip ./ && ...`

3、自带的推理服务启动命令 由`wget https://xxxx/xx/.zip` 部分改为 `cp /mnt/admin/offline/xx.zip ./`


